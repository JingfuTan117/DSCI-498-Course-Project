{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, WeightedRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "from torchmetrics.image.fid import FID\n",
    "from torchmetrics.image.kid import KID\n",
    "\n",
    "# -----------------------------\n",
    "# CVaR Utility\n",
    "# -----------------------------\n",
    "def cvar_loss(losses: torch.Tensor, alpha: float) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Approximate CVaR_alpha(L) = mean of the top (1-alpha) fraction of losses.\n",
    "    \"\"\"\n",
    "    sorted_losses, _ = torch.sort(losses, descending=True)\n",
    "    k = max(1, int((1 - alpha) * len(sorted_losses)))\n",
    "    tail = sorted_losses[:k]\n",
    "    return tail.mean()\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset Loaders\n",
    "# -----------------------------\n",
    "def get_imbalanced_mnist(imbalance_ratio: float, batch_size: int,\n",
    "                         baseline: str = 'none', oversample: bool = False):\n",
    "    \"\"\"\n",
    "    Returns train and test loaders for MNIST with class imbalance.\n",
    "    imbalance_ratio: fraction of samples to keep for classes 5-9 (minority)\n",
    "    baseline: 'none', 'reweight', or 'focal'\n",
    "    oversample: if True, uses WeightedRandomSampler to oversample minority\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    full_train = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
    "    test_loader = DataLoader(full_train, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    idx_major, idx_minor = [], []\n",
    "    for i, (_, label) in enumerate(full_train):\n",
    "        if label < 5:\n",
    "            idx_major.append(i)\n",
    "        else:\n",
    "            idx_minor.append(i)\n",
    "    n_minor = int(len(full_train) * imbalance_ratio / 2)\n",
    "    idx_minor = random.sample(idx_minor, n_minor)\n",
    "\n",
    "    selected_idx = idx_major + idx_minor\n",
    "    subset = Subset(full_train, selected_idx)\n",
    "\n",
    "    if oversample:\n",
    "        labels = [full_train[i][1] for i in selected_idx]\n",
    "        class_counts = torch.bincount(torch.tensor(labels), minlength=10).float()\n",
    "        weights = 1.0 / class_counts\n",
    "        sample_weights = weights[labels]\n",
    "        sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "        loader = DataLoader(subset, batch_size=batch_size, sampler=sampler)\n",
    "    else:\n",
    "        loader = DataLoader(subset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return loader, test_loader\n",
    "\n",
    "def get_celeba_rare(attr1: str, attr2: str, rare_ratio: float,\n",
    "                    batch_size: int, oversample: bool = False):\n",
    "    \"\"\"\n",
    "    Returns train loader for CelebA with rare attribute combination.\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.CenterCrop(178),\n",
    "        transforms.Resize(64),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    celeba_root = 'data/celeba'\n",
    "    full = datasets.CelebA(root=celeba_root, split='all', target_type='attr',\n",
    "                           download=True, transform=transform)\n",
    "    attr_idx = full.attr_names.index\n",
    "    rare_idx = [i for i, a in enumerate(full) if a[1][attr_idx(attr1)] == 1 and a[1][attr_idx(attr2)] == 1]\n",
    "    rest_idx = list(set(range(len(full))) - set(rare_idx))\n",
    "\n",
    "    n_rare = len(rare_idx)\n",
    "    n_rest = int(n_rare / rare_ratio)\n",
    "    rest_idx = random.sample(rest_idx, n_rest)\n",
    "\n",
    "    selected = rare_idx + rest_idx\n",
    "    subset = Subset(full, selected)\n",
    "    if oversample:\n",
    "        labels = [1 if i in rare_idx else 0 for i in selected]\n",
    "        counts = torch.bincount(torch.tensor(labels), minlength=2).float()\n",
    "        weights = 1.0 / counts\n",
    "        sample_weights = weights[labels]\n",
    "        sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "        loader = DataLoader(subset, batch_size=batch_size, sampler=sampler)\n",
    "    else:\n",
    "        loader = DataLoader(subset, batch_size=batch_size, shuffle=True)\n",
    "    return loader\n",
    "\n",
    "# -----------------------------\n",
    "# Models\n",
    "# -----------------------------\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=20):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(28*28, 400),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.mu_layer = nn.Linear(400, latent_dim)\n",
    "        self.logvar_layer = nn.Linear(400, latent_dim)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 28*28),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        return self.mu_layer(h), self.logvar_layer(h)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = (0.5 * logvar).exp()\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def decode(self, z):\n",
    "        out = self.decoder(z)\n",
    "        return out.view(-1, 1, 28, 28)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self.decode(z)\n",
    "        return x_recon, mu, logvar\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=100, channels=3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim, 512, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, channels, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        z = z.view(z.size(0), z.size(1), 1, 1)\n",
    "        return self.net(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels=3):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(channels, 64, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x).view(-1)\n",
    "\n",
    "# -----------------------------\n",
    "# Losses\n",
    "# -----------------------------\n",
    "def kl_divergence(mu, logvar):\n",
    "    return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)\n",
    "\n",
    "def focal_recon_loss(recon, x, gamma=2.0):\n",
    "    bce = nn.functional.binary_cross_entropy(recon, x, reduction='none').view(recon.size(0), -1).sum(dim=1)\n",
    "    p_t = torch.exp(-bce)\n",
    "    return ((1 - p_t) ** gamma * bce)\n",
    "\n",
    "# -----------------------------\n",
    "# Training Functions\n",
    "# -----------------------------\n",
    "def train_vae(model, dataloader, optimizer, device, alpha=None, baseline='none'):\n",
    "    model.train()\n",
    "    for x, labels in dataloader:\n",
    "        x = x.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon, mu, logvar = model(x)\n",
    "        bs = x.size(0)\n",
    "        recon_losses = None\n",
    "        if baseline == 'focal':\n",
    "            recon_losses = focal_recon_loss(recon, x)\n",
    "        else:\n",
    "            recon_losses = nn.functional.binary_cross_entropy(recon, x, reduction='none').view(bs, -1).sum(dim=1)\n",
    "\n",
    "        if baseline == 'reweight':\n",
    "            class_counts = torch.bincount(labels, minlength=10).float().to(device)\n",
    "            weights = 1.0 / class_counts[labels]\n",
    "            recon_losses = recon_losses * weights\n",
    "\n",
    "        if alpha is None:\n",
    "            recon_term = recon_losses.mean()\n",
    "        else:\n",
    "            recon_term = cvar_loss(recon_losses, alpha)\n",
    "        kl_term = kl_divergence(mu, logvar).mean()\n",
    "        loss = recon_term + kl_term\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "def train_gan(generator, discriminator, dataloader, optim_G, optim_D, device, alpha=None):\n",
    "    criterion = nn.ReLU()\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "    for x, _ in dataloader:\n",
    "        x = x.to(device)\n",
    "        bs = x.size(0)\n",
    "        optim_D.zero_grad()\n",
    "        real_scores = discriminator(x)\n",
    "        z = torch.randn(bs, 100, device=device)\n",
    "        fake = generator(z).detach()\n",
    "        fake_scores = discriminator(fake)\n",
    "        d_loss = torch.mean(criterion(1.0 - real_scores)) + torch.mean(criterion(1.0 + fake_scores))\n",
    "        d_loss.backward()\n",
    "        optim_D.step()\n",
    "\n",
    "        optim_G.zero_grad()\n",
    "        z = torch.randn(bs, 100, device=device)\n",
    "        fake = generator(z)\n",
    "        gen_losses = -discriminator(fake)\n",
    "        if alpha is None:\n",
    "            g_loss = gen_losses.mean()\n",
    "        else:\n",
    "            g_loss = cvar_loss(gen_losses, alpha)\n",
    "        g_loss.backward()\n",
    "        optim_G.step()\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluation\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def compute_fid(generator, real_loader, device):\n",
    "    fid = FID().to(device)\n",
    "    for x, _ in real_loader:\n",
    "        fid.update(x.to(device), real=True)\n",
    "    for x, _ in real_loader:\n",
    "        z = torch.randn(x.size(0), 100, device=device)\n",
    "        fid.update(generator(z), real=False)\n",
    "    return fid.compute().item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_kid(generator, real_loader, device):\n",
    "    kid = KID().to(device)\n",
    "    for x, _ in real_loader:\n",
    "        kid.update(x.to(device), real=True)\n",
    "    for x, _ in real_loader:\n",
    "        z = torch.randn(x.size(0), 100, device=device)\n",
    "        kid.update(generator(z), real=False)\n",
    "    return kid.compute().item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_coverage(generator, classifier, device, minority_labels, num_samples=10000):\n",
    "    classifier.eval()\n",
    "    count = 0\n",
    "    total = 0\n",
    "    for _ in range(num_samples // classifier.batch_size):\n",
    "        z = torch.randn(classifier.batch_size, 100, device=device)\n",
    "        fake = generator(z)\n",
    "        preds = classifier(fake).argmax(dim=1)\n",
    "        count += sum([1 for p in preds.cpu().numpy() if p in minority_labels])\n",
    "        total += len(preds)\n",
    "    return count / total\n",
    "\n",
    "# -----------------------------\n",
    "# Main Experiment\n",
    "# -----------------------------\n",
    "def main(args):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    train_loader, test_loader = get_imbalanced_mnist(\n",
    "        imbalance_ratio=args.imbal_balance_ratio if hasattr(args, 'imbal_balance_ratio') else args.imbalance_ratio,\n",
    "        batch_size=args.batch_size,\n",
    "        baseline=args.baseline,\n",
    "        oversample=(args.baseline == 'oversample')\n",
    "    )\n",
    "\n",
    "    classifier = resnet18(num_classes=10).to(device)\n",
    "    # TODO: load or train classifier here\n",
    "\n",
    "    results = {}\n",
    "    # VAE loop over alpha levels\n",
    "    for alpha in [None] + args.cvar_levels:\n",
    "        vae = VAE(latent_dim=args.latent_dim).to(device)\n",
    "        opt = optim.Adam(vae.parameters(), lr=args.lr)\n",
    "        for epoch in range(args.epochs):\n",
    "            train_vae(vae, train_loader, opt, device, alpha=alpha, baseline=args.baseline)\n",
    "        fid_score = compute_fid(vae.decode, test_loader, device)\n",
    "        kid_score = compute_kid(vae.decode, test_loader, device)\n",
    "        coverage = compute_coverage(vae.decode, classifier, device, minority_labels=list(range(5,10)))\n",
    "        results[f'VAE_alpha_{alpha}'] = {'FID': fid_score, 'KID': kid_score, 'Coverage': coverage}\n",
    "\n",
    "    # GAN loop over alpha levels\n",
    "    celeba_loader = get_celeba_rare(\n",
    "        args.attr1, args.attr2,\n",
    "        rare_ratio=args.rare_ratio,\n",
    "        batch_size=args.batch_size,\n",
    "        oversample=(args.baseline == 'oversample')\n",
    "    )\n",
    "    for alpha in [None] + args.cvar_levels:\n",
    "        G = Generator(latent_dim=args.latent_dim, channels=3).to(device)\n",
    "        D = Discriminator(channels=3).to(device)\n",
    "        optG = optim.Adam(G.parameters(), lr=args.lr, betas=(0.5, 0.999))\n",
    "        optD = optim.Adam(D.parameters(), lr=args.lr, betas=(0.5, 0.999))\n",
    "        for epoch in range(args.epochs):\n",
    "            train_gan(G, D, celeba_loader, optG, optD, device, alpha=alpha)\n",
    "        fid_score = compute_fid(G, celeba_loader, device)\n",
    "        kid_score = compute_kid(G, celeba_loader, device)\n",
    "        coverage = None  # placeholder for attribute coverage\n",
    "        results[f'GAN_alpha_{alpha}'] = {'FID': fid_score, 'KID': kid_score, 'Coverage': coverage}\n",
    "\n",
    "    # Summary printout\n",
    "    for key, val in results.items():\n",
    "        print(f\"{key}: FID={val['FID']:.2f}, KID={val['KID']:.4f}, Coverage={val['Coverage']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = ArgumentParser()\n",
    "    parser.add_argument('--imbalance_ratio', type=float, default=0.2)\n",
    "    parser.add_argument('--rare_ratio', type=float, default=0.1)\n",
    "    parser.add_argument('--batch_size', type=int, default=128)\n",
    "    parser.add_argument('--latent_dim', type=int, default=100)\n",
    "    parser.add_argument('--lr', type=float, default=2e-4)\n",
    "    parser.add_argument('--epochs', type=int, default=50)\n",
    "    parser.add_argument('--baseline', choices=['none', 'reweight', 'focal', 'oversample'], default='none')\n",
    "    parser.add_argument('--cvar_levels', nargs='+', type=float, default=[0.9, 0.95, 0.99])\n",
    "    parser.add_argument('--attr1', type=str, default='Wearing_Hat')\n",
    "    parser.add_argument('--attr2', type=str, default='Smiling')\n",
    "    args = parser.parse_args()\n",
    "    main(args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
